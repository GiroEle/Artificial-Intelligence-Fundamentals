{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Font Project Code.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"RXrQk0XyUvgg","colab_type":"text"},"source":["# APS360 Font Project \n","## By: Giro and Sagnik"]},{"cell_type":"code","metadata":{"id":"ehoHeZpgPs9Q","colab_type":"code","outputId":"12034042-18d4-46c8-cbdb-d5aab90a074e","executionInfo":{"status":"ok","timestamp":1565478195009,"user_tz":240,"elapsed":35516,"user":{"displayName":"Giro Liga","photoUrl":"https://lh4.googleusercontent.com/-Ht0XqYAxlNQ/AAAAAAAAAAI/AAAAAAAAAsk/-iQfTn-1e_U/s64/photo.jpg","userId":"09540742205704897669"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["import numpy as np\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import torch.optim as optim\n","import torchvision\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import torchvision.transforms as transforms\n","import os\n","import random\n","\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RkNrne6EhVpC","colab_type":"code","colab":{}},"source":["class LargeNet(nn.Module):\n","    def __init__(self):\n","        super(LargeNet, self).__init__()\n","        self.name = \"large\"\n","        self.conv1 = nn.Conv2d(3, 5, 5)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(5, 10, 5)\n","        self.fc1 = nn.Linear(10 * 53 * 53, 6200)\n","        self.fc2 = nn.Linear(6200, 128)\n","        self.fc3 = nn.Linear(128, 9)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = x.view(-1, 10 * 53 * 53)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        x = x.squeeze(1) # Flatten to [batch_size]\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2H9UrStyjG-3","colab_type":"code","outputId":"6f8cd691-2680-4bac-ae15-7b306da7c09f","executionInfo":{"status":"error","timestamp":1565478195359,"user_tz":240,"elapsed":35860,"user":{"displayName":"Giro Liga","photoUrl":"https://lh4.googleusercontent.com/-Ht0XqYAxlNQ/AAAAAAAAAAI/AAAAAAAAAsk/-iQfTn-1e_U/s64/photo.jpg","userId":"09540742205704897669"}},"colab":{"base_uri":"https://localhost:8080/","height":340}},"source":["###############################################################################\n","# Training\n","def get_model_name(name, batch_size, learning_rate, epoch):\n","    \"\"\" Generate a name for the model consisting of all the hyperparameter values\n","\n","    Args:\n","        config: Configuration object containing the hyperparameters\n","    Returns:\n","        path: A string with the hyperparameter name and value concatenated\n","    \"\"\"\n","    path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name,\n","                                                   batch_size,\n","                                                   learning_rate,\n","                                                   epoch)\n","    return path\n","\n","def normalize_label(labels):\n","    \"\"\"\n","    Given a tensor containing 2 possible values, normalize this to 0/1\n","\n","    Args:\n","        labels: a 1D tensor containing two possible scalar values\n","    Returns:\n","        A tensor normalize to 0/1 value\n","    \"\"\"\n","    max_val = torch.max(labels)\n","    min_val = torch.min(labels)\n","    norm_labels = (labels - min_val)/(max_val - min_val)\n","    return norm_labels\n","\n","def evaluate(net, loader, criterion):\n","    \"\"\" Evaluate the network on the validation set.\n","\n","     Args:\n","         net: PyTorch neural network object\n","         loader: PyTorch data loader for the validation set\n","         criterion: The loss function\n","     Returns:\n","         err: A scalar for the avg classification error over the validation set\n","         loss: A scalar for the average loss function over the validation set\n","     \"\"\"\n","    total_loss = 0.0\n","    total_err = 0.0\n","    total = 0\n","    total_epoch = 0\n","    for i, data in enumerate(loader, 0):\n","        inputs, labels = data\n","        outputs = net(inputs)\n","        loss = criterion(outputs, labels)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        total_err += (predicted != labels).sum().item()\n","        total_loss += loss.item()\n","        total_epoch += len(labels)\n","    err = float(total_err) / total_epoch\n","    loss = float(total_loss) / (i + 1)\n","    return err, loss\n","\n","###############################################################################\n","# Training Curve\n","def plot_training_curve(path):\n","    \"\"\" Plots the training curve for a model run, given the csv files\n","    containing the train/validation error/loss.\n","\n","    Args:\n","        path: The base path of the csv files produced during training\n","    \"\"\"\n","    import matplotlib.pyplot as plt\n","    train_err = np.loadtxt(\"{}_train_err.csv\".format(path))\n","    val_err = np.loadtxt(\"{}_val_err.csv\".format(path))\n","    train_loss = np.loadtxt(\"{}_train_loss.csv\".format(path))\n","    val_loss = np.loadtxt(\"{}_val_loss.csv\".format(path))\n","    plt.title(\"Train vs Validation Error\")\n","    n = len(train_err) # number of epochs\n","    plt.plot(range(1,n+1), train_err, label=\"Train\")\n","    plt.plot(range(1,n+1), val_err, label=\"Validation\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Error\")\n","    plt.legend(loc='best')\n","    plt.show()\n","    plt.title(\"Train vs Validation Loss\")\n","    plt.plot(range(1,n+1), train_loss, label=\"Train\")\n","    plt.plot(range(1,n+1), val_loss, label=\"Validation\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend(loc='best')\n","    plt.show()\n","\n","\n","def get_data_loader(batch_size):\n","    \"\"\" Returns the the shuffled and loaded dataset for training_set, validation_set and \n","    evaluation_set.\n","\n","    Args:\n","        batch_size\n","    Returns:\n","        train_loader\n","        val_loader\n","        test_loader\n","    \"\"\"\n","    ########################################################################\n","    # The output of torchvision datasets are PILImage images of range [0, 1].\n","    # We transform them to Tensors of normalized range [-1, 1].\n","    transform = transforms.Compose(\n","            [transforms.ToTensor(),\n","             transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","    dataset = torchvision.datasets.ImageFolder(root = '/content/drive/My Drive/fontData/fontDataSmallSet', transform = transform)\n","    random.shuffle(dataset.imgs)\n","    \n","    train_i = int(0.6*len(dataset))\n","    val_i = int(0.8*len(dataset))\n","    \n","    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n","    val_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n","    test_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n","    \n","    train_loader.dataset.imgs = train_loader.dataset.imgs[:train_i]\n","    val_loader.dataset.imgs = val_loader.dataset.imgs[train_i:val_i]\n","    test_loader.dataset.imgs = test_loader.dataset.imgs[val_i:]\n","    \n","    return train_loader.dataset.imgs ,val_loader.dataset.imgs ,test_loader.dataset.imgs \n","\n","#train,val,test = get_data_loader(32)\n","train,val,test = get_data_loader(16)\n"],"execution_count":0,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-98fde52098ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;31m#train,val,test = get_data_loader(32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-3-98fde52098ee>\u001b[0m in \u001b[0;36mget_data_loader\u001b[0;34m(batch_size)\u001b[0m\n\u001b[1;32m    103\u001b[0m             [transforms.ToTensor(),\n\u001b[1;32m    104\u001b[0m              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/My Drive/fontData/fontDataSmallSet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    207\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                                           is_valid_file=is_valid_file)\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m_find_classes\u001b[0;34m(self, dir)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# Faster and available in Python 3.5 and above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/fontData/fontDataSmallSet'"]}]},{"cell_type":"code","metadata":{"id":"55hY50wFl8vs","colab_type":"code","colab":{}},"source":["train"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"S5SO5ZGdi5Zq","colab_type":"code","colab":{}},"source":["def train(model, data, batch_size=4, num_epochs=100, learning_rate=0.01, small=False):\n","    # Fixed PyTorch random seed for reproducible result\n","    torch.manual_seed(1000) \n","    # load batches of the datasets\n","    train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size)\n","    # Define the Loss function and optimizer\n","    # The loss function will be Cross Entropy\n","    # Optimizer will adam\n","    criterion = nn.CrossEntropyLoss()     \n","    optimizer=optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    ########################################################################\n","    # Set up some numpy arrays to store the training/test loss/erruracy\n","    train_err = np.zeros(num_epochs)\n","    train_loss = np.zeros(num_epochs)\n","    val_err = np.zeros(num_epochs)\n","    val_loss = np.zeros(num_epochs)\n","    ########################################################################\n","    # Train the network\n","    # Loop over the data iterator and sample a new batch of training data\n","    # Get the output from the network, and optimize our loss function.\n","    start_time = time.time()\n","    \n","    iters, losses, train_acc, val_acc = [], [], [], []\n","    \n","    # training\n","    n = 0 # the number of iterations\n","    for epoch in range(num_epochs): # loop over the dataset multiple times\n","        for imgs, labels in iter(train_loader):\n","            #print(\"labels: \", labels)\n","            out = model(imgs)             # forward pass\n","            #print(\"outs: \", out)\n","            loss = criterion(out, labels) # compute the total loss\n","           \n","            loss.backward()               # backward pass (compute parameter updates)\n","            optimizer.step()              # make the updates for each parameter\n","            optimizer.zero_grad()         # a clean up step for PyTorch\n","\n","            # save the current training information\n","            iters.append(n)\n","       \n","            losses.append(float(loss)/batch_size)             # compute *average* loss\n","            #\n","            train_acc.append(get_accuracy(model, True, batch_size,small)) # compute training accuracy \n","            val_acc.append(get_accuracy(model, False, batch_size,small))  # compute validation accuracy\n","            n += 1\n","        print(\"-\")\n","        \n","        #creating setpoints every epouch\n","        model_path = get_model_name(model.name, batch_size, learning_rate, epoch)\n","        torch.save(model.state_dict(), model_path)"],"execution_count":0,"outputs":[]}]}